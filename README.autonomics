The autonomics pipeline is described in the paper XXXXXX.
Here we describe how to install and use the autonomics pipeline (pipeline).

----------------
THE INSTALLATION
----------------

The pipeline is intended to be run on a linux computer with multiple
nodes (the local computer).  In addition, a HPC cluster (referred to
as the HPC or remote computer) is required to run blast, bowtie and
hmmer. The pipeline code, including the mysql database, the
preassembly aps (adapter_trim, quality_trim (cutadapt), khmer (read
normalization), the assemblers (Trinity & mira), and the code to
create the GO & KEGG results run on the local machine.  The assemblers
use multiple nodes; the other aps use a single cpu.  The default
arguments to the various aps are given in the mysql table:
default_args.  For nondefault cases the args are inserted into the
args table (see how to manually insert a project into the pipeline,
below).

To start, edit the settings.py file in the autonomics directory and
enter data appropriate for your situation.  The settings file is
clearly documented to indicate what you need to modify.  Create the
necessary directories.  Some of the aps you will need installed on
your local machine are: khmer, trinity, mira, cutadapt, redis server,
ssh server, python 2.7.x, MySQL 5+, biopython, MySQL-python, paramiko,
pg8000, sqlalchemy; and on your remote cluster: pfam, blast, hmmer.

It may require some trial and error to set the wall time for blast NR
jobs as this will depend on your HPC cluster performance and the size
of your projects are, i.e. number of contigs in the assembled data
fasta file.  While setting wall time very high is safe, this may incur
a penalty in setting the priority of your jobs in the submission
queue.

Set up the mySQL database for the autonomics pipeline.  Here is how we did it:

% mysql -u root --password=XXXXX --database=mysql
mysql> create database zero_click;
mysql> create user zeroclick;
mysql> GRANT SELECT, INSERT, UPDATE, DELETE, INDEX, LOCK TABLES ON `zero_click`.* TO 'zeroclick'@'localhost';
mysql> GRANT SELECT, INSERT, UPDATE, DELETE, INDEX, LOCK TABLES ON `zero_click`.* TO 'zeroclick'@'127.0.0.1';
mysql> GRANT FILE ON *.* TO 'zero_click'@'localhost';
mysql> GRANT FILE ON *.* TO 'zero_click'@'127.0.0.1';
mysql> use mysql;
mysql> update user set Password=PASSWORD('XXXXXXX') where User like 'zeroclick%';
mysql> flush privileges;
mysql> use zero_click;

Now create the necessary tables in your mySQL database on your local
machine. (see autonomics/docs/README).  Then load the data to
initialize the tables. Here is what I did, right after the above commands.
In the autonomics directory:
mysql -u zeroclick -p zero_click < autonomics/data/autonomics.sql.tables
mysql -u zeroclick -p zero_click < autonomics/datas/autonomics.data.for.sql.tables

A redis keystore is used to speed up the annotation of Gene Ontology
and Kyoto Encyclopedia of Genes and Genomes terms.  First, download
and install the redis keystore from http://redis.io/ Then download
dump.rdb from our public ftp site.  Stop your redis keystore if it is
running, transfer the dump file to your redis directory, and restart
the redis server. The server should read in the dump file and populate
the key store with the necessary annotation mappings.  An example of
this procedure can be found at: http://goo.gl/Db68H (stackoverflow)

The code in autonomics/jobs.py creates qsub files that control jobs on
the remote hpc cluster.  Our cluster uses the moab control system and
the qsubs look like this:

#! /bin/bash
#PBS -r n
#PBS -N Capitella_proteins_pfam96
#PBS -o Capitella_proteins_pfam96.stdout
#PBS -e Capitella_proteins_pfam96.stderr
#PBS -q bio
#PBS -m a
#PBS -M morozhpc@gmail.com,plw1080@gmail.com
#PBS -l walltime=48:00:00
#PBS -l pmem=500mb
#PBS -l nodes=1:ppn=1
module load hmmer
pfam_scan.pl  -dir /scratch/lfs/moroz/pfam/ -cpu 1 -fasta /scratch/lfs/moroz/Capitella_proteins_pfam20131028171739/Capitella_proteins_project_pfam_96.fasta

The above script is created automatically by the autonomics pipeline code.
It gets it values such the email addresses from the settingpy file you
edited above.  You do not need to do anythin, we are just showing you
what it looks like for your information.

However, if your cluster does not use qsub scripts (most clusters do)
to run cluster batch jobs, edit the code in jobs.py so it creates qsub
files appropriate for your hpc cluster.


---------------------
STARTING THE PIPELINE
---------------------

After installing all of the above aps and modifying the settings.py
file, start the pipeline by executing start.auto in the autonomics
directory.  You will be asked to give a log file name.  This will then
start the dispatcher and manager loops which run continuously in the
background and control the pipeline.  The processes monitor the mySQL
database for jobs to run and the local and remote cluster for job
completion.  If you wish to use the data gremplin, then execute python
data_gremlin.py &.

----------------
THE DATA GREMLIN
----------------

The data-gremlin unit listens on the sequencing server and when a
project is completed, starts the project on its journey through
pipeline.  In its journey the reads will be assembled and then
annotated using blast (NR & SwissPort) and hmmer for the Pfams.  Based
on the SwissProt results, the Gene Ontology (GO) and KEGG results are
computed.  In addition, quantification (abundances) are computed using
bowtie.

At our lab, we have chosen not to use the data-gremlin as the lead
scientist wishes to evalaute the quality and validity of sequenced
projects before assembling/annotating them.  In addition, the default
names given to a project by the sequencer can be meaningless,
e.g. R_2013_09_26_14_06_58_user_PRO-23.  In the curation process,
meaningful names are assigned to the projects.  This avoids clogging
up the pipeline with bad data and annotating data with meaningless
project names.

----------------------------------------------------
HOW TO MANUALLY INSERT A PROJECT INTO THE PIPELINE
----------------------------------------------------

For the reasons given above in the data gremlin section, we manually
insert projects into the pipeline.  To do that we first create a
directory with the name of the project <project> in the pipeline
directory, and then copy the reads into that directory, naming them
<project>.fastq or in the case of paired end reads, <project>.fastq
and <project>.fastq.end2.  This naming convention must be strictly
followed.  The assembled data needs to be named
<project>_project.fasta.  The pipeline code will automatically name
the assembled data that way.  If you wish to annotate already
assembled data, e.g. gene models, you must be sure to name them is
this way.  The pipeline directory must look like this:

<projectA>/<projectA>.fastq
<projectB>/<projectB>.fastq
<projectB>/<projectB>.fastq.end2
<projectG>/<projectG>_project.fasta

Where <projectA> might be Aplysia_californica_gastrulation for example.
The use of fairly short and meaningful names in essential.

To manually initiate the assembly/annotation of a project, after
copying the reads to the pipeline directory as described above. Use
the ap: annotate.proj.pl:
usage: -proj :project_name (REQUIRED)
       -mira   (OPTIONAL, default is Trinity)
       -paired_end   (OPTIONAL, default is non-paired end)
       -noass   (OPTIONAL, use when the assembly step is to be omitted)
       -data   (OPTIONAL [ NT | AA ] - REQUIRED if -noass used)
The -noass case can be used for annotating gene models.

To sepcify even finer control of the args and aps used during the
annotation/assembly process the ap systemtools.py can be used instead
of annotate.proj.pl.  The interface to systemtools.py is complex; see
python systemtools.py --help systemtools.py is in the
autonomics/scripts directory.  99% of the time we use the simple
annotate.proj.pl to initiate jobs; when you execute it, you will see
how it in turn involves systemtools.py

When a project is started with annotate.proj.pl or systemtools.py, what
happens is the data for the project is entered into the mySQL database.
The manager and dispatcher processes described above then check the
data once a minute and any new jobs found there are started if their
dependencies are staisfied.

The dependencies are given in the dependency table:
mysql> select * from dependency;
+--------------------+--------------------+
| job_type           | depends_on         |
+--------------------+--------------------+
| assemble           | adapter_trim       |
| assemble           | quality_trim       |
| assemble           | read_normalization |
| blast_nr           | assemble           |
| blast_swissprot    | assemble           |
| go                 | assemble           |
| go                 | blast_swissprot    |
| kegg               | assemble           |
| kegg               | blast_swissprot    |
| pfam               | assemble           |
| quality_trim       | adapter_trim       |
| quantification     | assemble           |
| read_normalization | adapter_trim       |
| read_normalization | quality_trim       |
+--------------------+--------------------+

By default the jobs shown in the default_configuration table are run for a project:
mysql> select * from default_configuration;
+--------------------+------+
| job_type           | code |
+--------------------+------+
| adapter_trim       | +    |
| assemble           | +    |
| blast_nr           | +    |
| blast_swissprot    | +    |
| go                 | +    |
| kegg               | +    |
| pfam               | +    |
| quality_trim       | +    |
| quantification     | +    |
| read_normalization | +    |
+--------------------+------+


-----------
THE RESULTS
-----------

The pipeline puts the assembled data and annotations in the <project> directory
For example here is the data from a completed project:
% ls -1 Helix_lucorum_Control/
Helix_lucorum_Control.fastq          (the original reads)
Helix_lucorum_Control_project.fasta  (the assembled data)
Helix_lucorum_Control_blast_nr.txt
Helix_lucorum_Control_blast_swissprot.txt
Helix_lucorum_Control_gocats.txt
Helix_lucorum_Control_GO.txt
Helix_lucorum_Control_KEGG.txt
Helix_lucorum_Control_pfam.txt
Helix_lucorum_Control_quantification.txt

The gocats file is a GO category file containing 1) the number of
unique sequences placed into each GO category 2) the number of unique
annotations present in each category and 3) the abundance of all
sequences placed into that category.

The above files are the input to the neurobase comparative browser,
described in the paper YYYYYY.

--------------------------------
DEALING WITH PROBLEMS ON CLUSTER
--------------------------------

When running 100's of jobs concurrently on a large share cluster, as
the autonomics pipeline does, occasionally a job may fail due to a
node failure or some other hardware problem, or rarely because the
qsub deid not ask for enough wall time.  When this happens it usually
involves only one job out of the entire run. For example, when running
blast_nr for a project, the data is split into 100 parts and a
separate job is run for each part.

The term job is used in two senses: e.g. a blast_nr job for a project;
however a blast job is divided into 100 sub jobs and each run in parallel;
these sub-jobs are also referred to as jobs.

The procedure described below assumes some computational
sophistication on the part of the user.  If this is not the case, the
easiest, but not the fastest solution, is to just resubmit the entire
blast or pfam job.  You can do this by executing stop_proj.py <project_name>,
then restart.py <project_name> <job_type>.

When gettin started you may need to experiment with a few runs to see
how much wall time to give your jobs on the cluster as cluster vary in
performance.  You modify the wall time in settings.py After modifying
the settings file, you will need to stop the pipeline by killing all
processes running the dispatcher and the manager (use the ps command
and the kill -9 command).  Then restart the pipeline using start.auto.
You may need to restart any jobs (using restart.py) that were running
prior to this.

If a problem with a job is encountered the cluster should send an
email to the address specified in the settings.py file; otherwise if a
problem is suspected, log in to the cluster and run the qstat command
to see the status of all your jobs.  You can also go to the remote_dir
specified in the setting.py file and then to the dir with your run
data, e.g. Capitella_proteins_blast_nr20131028171733/.  A done flag
will be set for each job that has completed,
e.g. done.Capitella_proteins_blast_nr21. If you count these, for
example, for blast_nr jobs there should be 100 if all jobs completed.
If a job is missing, see it it is still running using qstat, if not,
look in its stderr file to see what the problem is.  If necessary you
can rerun any failed job by executing, e.g. qsub
Capitella_proteins_blast_nrXX.qsub where XX is the job number that
failed.  For successfully completed jobs, all jobs except the last job,
job 100 in the blast nr case, will have the ap countblast.pl to verify this.

gator1.ufhpc{plw1080}27: countblast.pl Capitella_proteins_blast_nr 1 100 nr
Capitella_proteins_blast_nr1_blast_nr.txt	325
Capitella_proteins_blast_nr2_blast_nr.txt	325
Capitella_proteins_blast_nr3_blast_nr.txt	325
Capitella_proteins_blast_nr4_blast_nr.txt	325
Capitella_proteins_blast_nr5_blast_nr.txt	325
Capitella_proteins_blast_nr6_blast_nr.txt	325
Capitella_proteins_blast_nr7_blast_nr.txt	325
..

In the rare case of a failed job on the cluster, after repairing it, you can
merge the blast outputs, Capitella_proteins_blast_nr6_blast_nr.txt for example
using the ap catall.blasts.pl, for example the command Capitella_proteins NR 100
will create one file, Capitella_proteins_blast_nr.txt with the merged results.
The scp that file to your project directory on your local machine.

If the problem encountered was that some jobs did not have enough wall
time, then edit the relevant qsub files and increase the wall time and
qsub them again.  If all the qsub files need an increase, use sed and
a command something like this: sed -i 's/old-wallTime/new-wallTime/g' *.qsub
This may be an indication that you need to increase the wall time in the
settings.py file - see above in the section The Installation.

